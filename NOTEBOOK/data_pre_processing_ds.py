# -*- coding: utf-8 -*-
"""DATA PRE-PROCESSING- DS

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-OoOu0NieSZnYdwWs9NLDl4oqMaEdHwZ

## **DATA PRE-PROCESSING**

### **Load Data**
"""

from google.colab import files
data_to_load = files.upload()

import pandas as pd
from matplotlib import pyplot as plt
from statsmodels.graphics.tsaplots import plot_acf

# data setup
import io
data = pd.read_csv(io.BytesIO(data_to_load['ORIGIN_DATA.csv']),parse_dates=["DATE"])
data.head()

# Import Necessary Libraries
import pandas as pd
import numpy as np
from sklearn import linear_model
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from matplotlib import pyplot as plt
from scipy.stats.mstats import zscore
import seaborn as sns
from pandas import Series
from matplotlib import pyplot
from statsmodels.tsa.stattools import adfuller
import itertools
import math
import random

df=data.copy()

"""### **Data Exploration**"""

# Histogram of all numeric fields
df_hist = df.drop(columns=['DATE'],axis=1)
df_hist.hist(figsize=(30,30));

a = df.iloc[:, 67:73]
a

a.plot(subplots=True)
plt.tight_layout()
plt.show()

import plotly.graph_objects as go
fig = go.Figure()
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,2], name = data.iloc[:,2].name, line = dict(color = '#FF0000'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,3], name = data.iloc[:,3].name, line = dict(color = '#CD853F'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,4], name = data.iloc[:,4].name, line = dict(color = '#483D8B'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,5], name = data.iloc[:,5].name, line = dict(color = '#00008B'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,6], name = data.iloc[:,6].name, line = dict(color = '#8B4513'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,7], name = data.iloc[:,7].name, line = dict(color = '#7B68EE'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,8], name = data.iloc[:,8].name, line = dict(color = '#800080'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,9], name = data.iloc[:,9].name, line = dict(color = '#87CEEB'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,10], name = data.iloc[:,10].name, line = dict(color = '#FFDAB9'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,11], name = data.iloc[:,11].name, line = dict(color = '#32CD32'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,12], name = data.iloc[:,12].name, line = dict(color = '#D8BFD8'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,13], name = data.iloc[:,13].name, line = dict(color = '#DDA0DD'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,14], name = data.iloc[:,14].name, line = dict(color = '#6495ED'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,15], name = data.iloc[:,15].name, line = dict(color = '#FFA07A'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,16], name = data.iloc[:,16].name, line = dict(color = '#4682B4'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,17], name = data.iloc[:,17].name, line = dict(color = '#556B2F'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,18], name = data.iloc[:,18].name, line = dict(color = '#9ACD32'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,19], name = data.iloc[:,19].name, line = dict(color = '#B0C4DE'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,20], name = data.iloc[:,20].name, line = dict(color = '#9370DB'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,21], name = data.iloc[:,21].name, line = dict(color = '#DAA520'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,22], name = data.iloc[:,22].name, line = dict(color = '#0000CD'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,23], name = data.iloc[:,23].name, line = dict(color = '#008080'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,24], name = data.iloc[:,24].name, line = dict(color = '#B8860B'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,25], name = data.iloc[:,25].name, line = dict(color = '#FFD700'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,26], name = data.iloc[:,26].name, line = dict(color = '#800000'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,27], name = data.iloc[:,27].name, line = dict(color = '#FF8C00'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,28], name = data.iloc[:,28].name, line = dict(color = '#B0E0E6'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,29], name = data.iloc[:,29].name, line = dict(color = '#FF00FF'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,30], name = data.iloc[:,30].name, line = dict(color = '#F4A460'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,31], name = data.iloc[:,31].name, line = dict(color = '#FFFF00'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,32], name = data.iloc[:,32].name, line = dict(color = '#D2691E'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,33], name = data.iloc[:,33].name, line = dict(color = '#7FFFD4'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,34], name = data.iloc[:,34].name, line = dict(color = '#0000FF'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,35], name = data.iloc[:,35].name, line = dict(color = '#FFEFD5'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,36], name = data.iloc[:,36].name, line = dict(color = '#EEE8AA'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,37], name = data.iloc[:,37].name, line = dict(color = '#FFA500'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,38], name = data.iloc[:,38].name, line = dict(color = '#9400D3'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,39], name = data.iloc[:,39].name, line = dict(color = '#FFDEAD'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,40], name = data.iloc[:,40].name, line = dict(color = '#66CDAA'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,41], name = data.iloc[:,41].name, line = dict(color = '#8B008B'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,42], name = data.iloc[:,42].name, line = dict(color = '#D2B48C'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,43], name = data.iloc[:,43].name, line = dict(color = '#ADFF2F'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,44], name = data.iloc[:,44].name, line = dict(color = '#008000'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,45], name = data.iloc[:,45].name, line = dict(color = '#F5DEB3'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,46], name = data.iloc[:,46].name, line = dict(color = '#8FBC8F'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,47], name = data.iloc[:,47].name, line = dict(color = '#DEB887'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,48], name = data.iloc[:,48].name, line = dict(color = '#FF00FF'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,49], name = data.iloc[:,49].name, line = dict(color = '#BDB76B'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,50], name = data.iloc[:,50].name, line = dict(color = '#9932CC'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,51], name = data.iloc[:,51].name, line = dict(color = '#87CEFA'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,52], name = data.iloc[:,52].name, line = dict(color = '#00FF00'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,53], name = data.iloc[:,53].name, line = dict(color = '#B22222'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,54], name = data.iloc[:,54].name, line = dict(color = '#E9967A'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,55], name = data.iloc[:,55].name, line = dict(color = '#E0FFFF'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,56], name = data.iloc[:,56].name, line = dict(color = '#6A5ACD'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,57], name = data.iloc[:,57].name, line = dict(color = '#FFE4B5'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,58], name = data.iloc[:,58].name, line = dict(color = '#FA8072'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,59], name = data.iloc[:,59].name, line = dict(color = '#4169E1'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,60], name = data.iloc[:,60].name, line = dict(color = '#E6E6FA'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,61], name = data.iloc[:,61].name, line = dict(color = '#1E90FF'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,62], name = data.iloc[:,62].name, line = dict(color = '#DA70D6'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,63], name = data.iloc[:,63].name, line = dict(color = '#F08080'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,64], name = data.iloc[:,64].name, line = dict(color = '#BC8F8F'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,65], name = data.iloc[:,65].name, line = dict(color = '#FF6347'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,66], name = data.iloc[:,66].name, line = dict(color = '#ADD8E6'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,67], name = data.iloc[:,67].name, line = dict(color = '#8B0000'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,68], name = data.iloc[:,68].name, line = dict(color = '#FFC0CB'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,69], name = data.iloc[:,69].name, line = dict(color = '#808000'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,70], name = data.iloc[:,70].name, line = dict(color = '#00FFFF'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,71], name = data.iloc[:,71].name, line = dict(color = '#C71585'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,72], name = data.iloc[:,72].name, line = dict(color = '#A0522D'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,73], name = data.iloc[:,73].name, line = dict(color = '#FF69B4'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,74], name = data.iloc[:,74].name, line = dict(color = '#00FF7F'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,75], name = data.iloc[:,75].name, line = dict(color = '#BA55D3'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,76], name = data.iloc[:,76].name, line = dict(color = '#000080'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,77], name = data.iloc[:,77].name, line = dict(color = '#00FFFF'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,78], name = data.iloc[:,78].name, line = dict(color = '#FF1493'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,79], name = data.iloc[:,79].name, line = dict(color = '#5F9EA0'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,80], name = data.iloc[:,80].name, line = dict(color = '#663399'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,81], name = data.iloc[:,81].name, line = dict(color = '#FAFAD2'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,82], name = data.iloc[:,82].name, line = dict(color = '#228B22'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,83], name = data.iloc[:,83].name, line = dict(color = '#8A2BE2'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,84], name = data.iloc[:,84].name, line = dict(color = '#00CED1'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,85], name = data.iloc[:,85].name, line = dict(color = '#EE82EE'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,86], name = data.iloc[:,86].name, line = dict(color = '#FF4500'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,87], name = data.iloc[:,87].name, line = dict(color = '#4B0082'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,88], name = data.iloc[:,88].name, line = dict(color = '#DB7093'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,89], name = data.iloc[:,89].name, line = dict(color = '#CD5C5C'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,90], name = data.iloc[:,90].name, line = dict(color = '#20B2AA'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,91], name = data.iloc[:,91].name, line = dict(color = '#F0E68C'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,92], name = data.iloc[:,92].name, line = dict(color = '#6B8E23'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,93], name = data.iloc[:,93].name, line = dict(color = '#FF7F50'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,94], name = data.iloc[:,94].name, line = dict(color = '#3CB371'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,95], name = data.iloc[:,95].name, line = dict(color = '#FFFACD'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,96], name = data.iloc[:,96].name, line = dict(color = '#98FB98'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,97], name = data.iloc[:,97].name, line = dict(color = '#FFE4C4'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,98], name = data.iloc[:,98].name, line = dict(color = '#FFFFE0'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,99], name = data.iloc[:,99].name, line = dict(color = '#A52A2A'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,100], name = data.iloc[:,100].name, line = dict(color = '#48D1CC'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,101], name = data.iloc[:,101].name, line = dict(color = '#2E8B57'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,102], name = data.iloc[:,102].name, line = dict(color = '#7CFC00'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,103], name = data.iloc[:,103].name, line = dict(color = '#40E0D0'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,104], name = data.iloc[:,104].name, line = dict(color = '#DC143C'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,105], name = data.iloc[:,105].name, line = dict(color = '#90EE90'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,106], name = data.iloc[:,106].name, line = dict(color = '#7FFF00'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,107], name = data.iloc[:,107].name, line = dict(color = '#FFB6C1'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,108], name = data.iloc[:,108].name, line = dict(color = '#00BFFF'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,109], name = data.iloc[:,109].name, line = dict(color = '#191970'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,110], name = data.iloc[:,110].name, line = dict(color = '#AFEEEE'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,111], name = data.iloc[:,111].name, line = dict(color = '#000000'), opacity = 1.0))
#fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,112], name = data.iloc[:,112].name, line = dict(color = '#000000'), opacity = 0.8))

# Understanding the numeric fields
quality=df.describe()
quality

"""### **Missing Values**"""

# Total missing values for each feature
df.isnull().sum().sort_values(ascending=False)/len(df)

def missing_values_table(df):
        # Total missing values
        mis_val = df.isnull().sum()
        
        # Percentage of missing values
        mis_val_percent = 100 * df.isnull().sum() / len(df)
        
        # Make a table with the results
        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)
        
        # Rename the columns
        mis_val_table_ren_columns = mis_val_table.rename(
        columns = {0 : 'Missing Values', 1 : '% of Total Values'})
        
        # Sort the table by percentage of missing descending
        mis_val_table_ren_columns = mis_val_table_ren_columns[
            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(
        '% of Total Values', ascending=False).round(1)
        
        # Print some summary information
        print ("Your selected dataframe has " + str(df.shape[1]) + " columns.\n"      
            "There are " + str(mis_val_table_ren_columns.shape[0]) +
              " columns that have missing values.")
        
        # Return the dataframe with missing information
        return mis_val_table_ren_columns

mis_val= missing_values_table(df)
mis_val

from google.colab import files
mis_val.to_csv('Missing value-report.csv') 
files.download('Missing value-report.csv')

# Delete columns containing either 10% or more than 10% NaN Values
perc = 10.0
min_count =  int(((100-perc)/100)*df.shape[0] + 1)
mod_df = df.dropna( axis=1, 
                thresh=min_count)

mod_df.shape

# Add a Quarterly variables back to DataFrame
q = df['ACTIVEJOBS']
q1 = df['JOBVACAN']
q2 = df['RETRENCH']
q3 = df['GDPGR']
mod_df = mod_df.join(q)
mod_df = mod_df.join(q1)
mod_df = mod_df.join(q2)
mod_df = mod_df.join(q3)

mod_df

"""### **Missing Values - MICE**"""

#MICE
import numpy as np 
import pandas as pd
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

df_2 = mod_df.copy()
df_3 = df_2.drop(['DATE'],axis=1)

imputer=IterativeImputer(imputation_order='ascending',max_iter=10,random_state=42,n_nearest_features=5)
imputer
imputed_dataset = imputer.fit_transform(df_3)

df_3 = pd.DataFrame(imputed_dataset, columns = ['ALRCB','CIC','CPI','CRUDEBRUSD','EXSITC0','EXSITC1','EXSITC2','EXSITC3','EXSITC4','EXSITC5','EXSITC6','EXSITC7','EXSITC8','EXSITC9','EXTOT','IMSITC0','IMSITC1','IMSITC2','IMSITC3','IMSITC4','IMSITC5','IMSITC6','IMSITC7','IMSITC8','IMSITC9','IMTOT','IPIELEC','IPIMFG','IPIMIN','IPITOT','LGC','LIC','TOURIST','USDEXC','SOPC','SALETAX','SERVTAX','CONSCRE','RUBBER','PALMOIL','IMPCON','OBOCC','ACTIVEJOBS','JOBVACAN','RETRENCH','GDPGR'])

print(df_3)
print(type(df_3))

df_3.head()

df_3.isnull().sum()

#Adding back DATE column

date = df['DATE']
df_3 = df_3.join(date)

df_3.head()

df_3.head()

from google.colab import files
df_3.to_csv('GDPGR_clean.csv') 
files.download('GDPGR_clean.csv')

"""### **Imputation -  nowcast_lstm**"""

!pip install nowcast_lstm
!pip install dill numpy pandas pmdarima

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import dill # for loading and saving a trained model
from nowcast_lstm.LSTM import LSTM
from nowcast_lstm.model_selection import variable_selection, hyperparameter_tuning, select_model

import warnings
warnings.filterwarnings("ignore")

from google.colab import files
data_to_load = files.upload()

# data setup
#import io
data = pd.read_csv(io.BytesIO(data_to_load['GDP_S1 (1).csv']),parse_dates=["DATE"])
variables = ['ALRCB','CIC','CPI','CRUDEBRUSD','EXSITC0','EXSITC1','EXSITC2','EXSITC3','EXSITC4','EXSITC5','EXSITC6','EXSITC7','EXSITC8','EXSITC9','EXTOT','IMSITC0','IMSITC1','IMSITC2','IMSITC3','IMSITC4','IMSITC5','IMSITC6','IMSITC7','IMSITC8','IMSITC9','IMTOT','IPIELEC','IPIMFG','IPIMIN','IPITOT','LGC','LIC','TOURIST','USDEXC','SOPC','SALETAX','SERVTAX','CONSCRE','RUBBER','PALMOIL','IMPCON','OBOCC','ACTIVEJOBS','JOBVACAN','RETRENCH']
target = ['GDPGR']

data.head()

#fill these using the mean of the series.a quarterly variable, has additionally had its within-series missings filled with the series mean as well
#tmp = LSTM(data,'GDPGR', n_timesteps=12, fill_na_func=np.nanmean).dataset["na_filled_dataset"][-10:,:-1] # pass fill_na_func=np.nanmedian to use the median
tmp = LSTM(df_2,'GDPGR', n_timesteps=12, fill_na_func=np.nanmean).dataset["na_filled_dataset"][:,:-1]
new2=pd.DataFrame(tmp, columns=variables)

#ragged edge problem arises with differences in publication lags among the variables. 
# ragged edges can also be filled by ARMA estimation, in the below example ARMA parameters were estimated using `pmdarima.arima.auto_arima` then fit to the data to fill ragged edges
# ragged edges have now been filled using ARMA models for each series. x_vol_world2 within-series missings continue to be filled in with the series mean as passed to the `fill_na_func`

tmp_2 = LSTM(df_2, "GDPGR", n_timesteps=12, fill_na_func=np.nanmean, fill_ragged_edges_func="ARMA").dataset["na_filled_dataset"][:,:-1]
new3=pd.DataFrame(tmp_2, columns=variables)

#Adding back GDPGR column
#numbers = df2["Numbers"]
#df1 = df1.join(numbers)

gdp = df_3['GDPGR']
new3 = new3.join(gdp)

from google.colab import files
new3.to_csv('GDP_LSTM-ARMA.csv') 
files.download('GDP_LSTM-ARMA.csv')

#Adding back GDPGR column
#numbers = df2["Numbers"]
#df1 = df1.join(numbers)

gdp = df_3['GDPGR']
new2 = new2.join(gdp)

from google.colab import files
new2.to_csv('GDP_LSTM-MEAN.csv') 
files.download('GDP_LSTM-MEAN.csv')

"""### **Diagnostic Statistics**"""

from google.colab import files
data_to_load = files.upload()

import pandas as pd
from matplotlib import pyplot as plt
from statsmodels.graphics.tsaplots import plot_acf

# data setup
import io
df_diag = pd.read_csv(io.BytesIO(data_to_load['GDPGR_clean.csv']),parse_dates=["DATE"])
df_diag.head()

# creating features and label variable
X = df_diag[['ALRCB','CIC','CPI','CRUDEBRUSD','EXSITC0','EXSITC1','EXSITC2','EXSITC3','EXSITC4','EXSITC5','EXSITC6','EXSITC7','EXSITC8','EXSITC9','EXTOT','IMSITC0','IMSITC1','IMSITC2','IMSITC3','IMSITC4','IMSITC5','IMSITC6','IMSITC7','IMSITC8','IMSITC9','IMTOT','IPIELEC','IPIMFG','IPIMIN','IPITOT','LGC','LIC','TOURIST','USDEXC','SOPC','SALETAX','SERVTAX','CONSCRE','RUBBER','PALMOIL','IMPCON','OBOCC','ACTIVEJOBS','JOBVACAN','RETRENCH']]
y = df_diag[["GDPGR"]]

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_scaled

# checking for multicollinearity using `VIF` and `correlation matrix`

from statsmodels.stats.outliers_influence import variance_inflation_factor

vif = pd.DataFrame()

vif['VIF'] = [variance_inflation_factor(X_scaled, i) for i in range(X_scaled.shape[1])]
vif['Features'] = X.columns

vif

from google.colab import files
vif.to_csv('VIF-report.csv') 
files.download('VIF-report.csv')

#correlation analysis
corr = df_diag.corr()
sns.heatmap(df_diag.corr());

from google.colab import files
corr.to_csv('Correlation-report.csv') 
files.download('Correlation-report.csv')

# Function to check stationarity

def adfuller_test(series, signif=0.05, name='', verbose=False):
    """Perform ADFuller to test for Stationarity of given series and print report"""
    r = adfuller(series, autolag='AIC')
    output = {'test_statistic':round(r[0], 4), 'pvalue':round(r[1], 4), 'n_lags':round(r[2], 4), 'n_obs':r[3]}
    p_value = output['pvalue'] 
    def adjust(val, length= 6): return str(val).ljust(length)

    # Print Summary
    print(f'    Augmented Dickey-Fuller Test on "{name}"', "\n   ", '-'*47)
    print(f' Null Hypothesis: Data has unit root. Non-Stationary.')
    print(f' Significance Level    = {signif}')
    print(f' Test Statistic        = {output["test_statistic"]}')
    print(f' No. Lags Chosen       = {output["n_lags"]}')

    for key,val in r[4].items():
        print(f' Critical value {adjust(key)} = {round(val, 3)}')

    if p_value <= signif:
        print(f" => P-Value = {p_value}. Rejecting Null Hypothesis.")
        print(f" => Series is Stationary.")
    else:
        print(f" => P-Value = {p_value}. Weak evidence to reject the Null Hypothesis.")
        print(f" => Series is Non-Stationary.")

# Data Prep

AI_Model_data = pd.concat([y, X], axis=1, sort=False)
#AI_Model_data = AI_Model_data.dropna()
# Check stationarity

# ADF Test on each column
for name, column in AI_Model_data.iteritems():
    adfuller_test(column, name=column.name)
    print('\n')

#Engineering features based on creating lags and perdiodic changes of features

df_4 = df_diag.copy()

pred1 = df_4[['ALRCB']]
pred1 = pred1.shift(2)
pred1 = pred1.add_suffix('_Lag2')

pred2 = df_4[['CIC']]
pred2 = pred2.shift(15)
pred2 = pred2.add_suffix('_Lag15')

pred3 = df_4[['CPI']]
pred3 = pred3.shift(2)
pred3 = pred3.add_suffix('_Lag2')

pred4 = df_4[['CRUDEBRUSD']]
pred4 = pred4.shift(1)
pred4 = pred4.add_suffix('_Lag1')

pred5 = df_4[['EXSITC0']]
pred5 = pred5.shift(14)
pred5 = pred5.add_suffix('_Lag14')

pred6 = df_4[['EXSITC1']]
pred6 = pred6.shift(14)
pred6 = pred6.add_suffix('_Lag14')

pred7 = df_4[['EXSITC2']]
pred7 = pred7.shift(11)
pred7 = pred7.add_suffix('_Lag11')

pred8 = df_4[['EXSITC3']]
pred8 = pred8.shift(1)
pred8 = pred8.add_suffix('_Lag1')

pred9 = df_4[['EXSITC4']]
pred9 = pred9.shift(8)
pred9 = pred9.add_suffix('_Lag8')

pred10 = df_4[['EXSITC5']]
pred10 = pred10.shift(13)
pred10 = pred10.add_suffix('_Lag13')

pred11 = df_4[['EXSITC6']]
pred11 = pred11.shift(4)
pred11 = pred11.add_suffix('_Lag4')

pred12 = df_4[['EXSITC7']]
pred12 = pred12.shift(12)
pred12 = pred12.add_suffix('_Lag12')

pred13 = df_4[['EXSITC8']]
pred13 = pred13.shift(14)
pred13 = pred13.add_suffix('_Lag14')

pred14 = df_4[['EXSITC9']]
pred14 = pred14.shift(3)
pred14 = pred14.add_suffix('_Lag3')

pred15 = df_4[['EXTOT']]
pred15 = pred15.shift(13)
pred15 = pred15.add_suffix('_Lag13')

pred16 = df_4[['IMSITC0']]
pred16 = pred16.shift(11)
pred16 = pred16.add_suffix('_Lag11')

pred17 = df_4[['IMSITC1']]
pred17 = pred17.shift(13)
pred17 = pred17.add_suffix('_Lag13')

pred18 = df_4[['IMSITC2']]
pred18 = pred18.shift(14)
pred18 = pred18.add_suffix('_Lag14')

pred19 = df_4[['IMSITC3']]
pred19 = pred19.shift(2)
pred19 = pred19.add_suffix('_Lag2')

pred20 = df_4[['IMSITC4']]
pred20 = pred20.shift(11)
pred20 = pred20.add_suffix('_Lag11')

pred21 = df_4[['IMSITC5']]
pred21 = pred21.shift(12)
pred21 = pred21.add_suffix('_Lag12')

pred22 = df_4[['IMSITC6']]
pred22 = pred22.shift(13)
pred22 = pred22.add_suffix('_Lag13')

pred23 = df_4[['IMSITC7']]
pred23 = pred23.shift(13)
pred23 = pred23.add_suffix('_Lag13')

pred24 = df_4[['IMSITC8']]
pred24 = pred24.shift(12)
pred24 = pred24.add_suffix('_Lag12')

pred25 = df_4[['IMSITC9']]
pred25 = pred25.shift(12)
pred25 = pred25.add_suffix('_Lag12')

pred26 = df_4[['IMTOT']]
pred26 = pred26.shift(14)
pred26 = pred26.add_suffix('_Lag14')

pred27 = df_4[['IPIELEC']]
pred27 = pred27.shift(13)
pred27 = pred27.add_suffix('_Lag13')

pred28 = df_4[['IPIMFG']]
pred28 = pred28.shift(14)
pred28 = pred28.add_suffix('_Lag14')

pred29 = df_4[['IPIMIN']]
pred29 = pred29.shift(12)
pred29 = pred29.add_suffix('_Lag12')

pred30 = df_4[['IPITOT']]
pred30 = pred30.shift(14)
pred30 = pred30.add_suffix('_Lag14')

pred31 = df_4[['LGC']]
pred31 = pred31.shift(15)
pred31 = pred31.add_suffix('_Lag15')

pred32 = df_4[['LIC']]
pred32 = pred32.shift(15)
pred32 = pred32.add_suffix('_Lag15')

pred33 = df_4[['TOURIST']]
pred33 = pred33.shift(12)
pred33 = pred33.add_suffix('_Lag12')

pred34 = df_4[['USDEXC']]
pred34 = pred34.shift(1)
pred34 = pred34.add_suffix('_Lag1')

pred35 = df_4[['SOPC']]
pred35 = pred35.shift(13)
pred35 = pred35.add_suffix('_Lag13')

pred36 = df_4[['SALETAX']]
pred36 = pred36.shift(7)
pred36 = pred36.add_suffix('_Lag7')

pred37 = df_4[['SERVTAX']]
pred37 = pred37.shift(7)
pred37 = pred37.add_suffix('_Lag7')

pred38 = df_4[['CONSCRE']]
pred38 = pred38.shift(12)
pred38 = pred38.add_suffix('_Lag12')

pred39 = df_4[['RUBBER']]
pred39 = pred39.shift(12)
pred39 = pred39.add_suffix('_Lag12')

pred40 = df_4[['PALMOIL']]
pred40 = pred40.shift(14)
pred40 = pred40.add_suffix('_Lag14')

pred41 = df_4[['IMPCON']]
pred41 = pred41.shift(14)
pred41 = pred41.add_suffix('_Lag14')

pred42 = df_4[['OBOCC']]
pred42 = pred42.shift(12)
pred42 = pred42.add_suffix('_Lag12')

pred43 = df_4[['ACTIVEJOBS']]
pred43 = pred43.shift(14)
pred43 = pred43.add_suffix('_Lag14')

pred44 = df_4[['JOBVACAN']]
pred44 = pred44.shift(5)
pred44 = pred44.add_suffix('_Lag5')

pred45 = df_4[['RETRENCH']]
pred45 = pred45.shift(14)
pred45 = pred45.add_suffix('_Lag14')

predictors = pd.concat([pred1, pred2, pred3, pred4, pred5, pred6, pred7, pred8, pred9, pred10, pred11, pred12, pred13, pred14, pred15, pred16, pred17, pred18, pred19, pred20, pred21, pred22, pred23, pred24, pred25, pred26, pred27, pred28, pred29, pred30, pred31, pred32, pred33, pred34, pred35, pred36, pred37, pred38, pred39, pred40, pred41, pred42, pred43, pred44, pred45] , axis=1) 

#predictors=predictors.dropna(axis=0, how='any')

predictors.head()

from google.colab import files
predictors.to_csv('GDPGR_lag.csv') 
files.download('GDPGR_lag.csv')

"""### **Feature Selection**"""

predictors = df_3[['ALRCB','CIC','CPI','CRUDEBRUSD','EXSITC0','EXSITC1','EXSITC2','EXSITC3','EXSITC4','EXSITC5','EXSITC6','EXSITC7','EXSITC8','EXSITC9','EXTOT','IMSITC0','IMSITC1','IMSITC2','IMSITC3','IMSITC4','IMSITC5','IMSITC6','IMSITC7','IMSITC8','IMSITC9','IMTOT','IPIELEC','IPIMFG','IPIMIN','IPITOT','LGC','LIC','TOURIST','USDEXC','SOPC','SALETAX','SERVTAX','CONSCRE','RUBBER','PALMOIL','IMPCON','OBOCC','ACTIVEJOBS','JOBVACAN','RETRENCH']]
Target = df_3[["GDPGR"]]

#1.Select the top n features based on feature importance from random forest

np.random.seed(10)

# define the model
model = RandomForestRegressor(random_state = random.seed(10))
# fit the model
model.fit(predictors, Target)

# get importance
features = predictors
importances = model.feature_importances_
indices = np.argsort(importances)

feat_importances = pd.Series(model.feature_importances_, index=predictors.columns)
feat_importances.nlargest(30).plot(kind='barh')

#Final Features from Random Forest (Select Features with highest feature importance)
rf_top_features = pd.DataFrame(feat_importances.nlargest(47)).axes[0].tolist()
rf_top_features

#2.Select the top n features based on absolute correlation with target variable
corr_data1 = pd.concat([Target,predictors],axis = 1)
corr_data = corr_data1.corr()
corr_data = corr_data.iloc[: , [0]]
corr_data.columns.values[0] = "Correlation"
corr_data = corr_data.iloc[corr_data.Correlation.abs().argsort()] 
corr_data = corr_data[corr_data['Correlation'].notna()]
corr_data = corr_data.loc[corr_data['Correlation'] != 1]
corr_data.tail(20)

# Select Features with greater than 60% absolute correlation
corr_data2 = corr_data.loc[corr_data['Correlation'].abs() > 0.5]
corr_top_features = corr_data2.axes[0].tolist()
corr_top_features

sns.heatmap(df.corr());

#3.Select the features identified by Lasso regression

np.random.seed(10)

estimator = LassoCV(cv=5, normalize = True)

sfm = SelectFromModel(estimator, prefit=False, norm_order=1, max_features=None)

sfm.fit(predictors, Target)

feature_idx = sfm.get_support()
Lasso_features = predictors.columns[feature_idx].tolist()
Lasso_features

#4.Perform recursive feature selection and use cross validation to identify the best number of features

#Feature ranking with recursive feature elimination and cross-validated selection of the best number of features

rfe_selector = RFE(estimator=LinearRegression(), n_features_to_select= 47, step=10, verbose=5)
rfe_selector.fit(predictors, Target)
rfe_support = rfe_selector.get_support()
rfe_feature = predictors.loc[:,rfe_support].columns.tolist()
rfe_feature

#5.Select the top n features based on absolute value of beta coefficients of features

# define standard scaler
scaler = StandardScaler()
# transform x data
scaled_predictors = scaler.fit_transform(predictors)
scaled_Target = scaler.fit_transform(Target)

sr_reg = LinearRegression(fit_intercept = False).fit(scaled_predictors, scaled_Target)
coef_table = pd.DataFrame(list(predictors.columns)).copy()
coef_table.insert(len(coef_table.columns),"Coefs",sr_reg.coef_.transpose())
coef_table = coef_table.iloc[coef_table.Coefs.abs().argsort()] 


sr_data2 = coef_table.tail(55)
sr_top_features = sr_data2.iloc[:,0].tolist()
sr_top_features

# Combining features from all the models

combined_feature_list = sr_top_features + Lasso_features + corr_top_features + rf_top_features

combined_feature = {x:combined_feature_list.count(x) for x in combined_feature_list}
combined_feature_data = pd.DataFrame.from_dict(combined_feature,orient='index')

combined_feature_data.rename(columns={ combined_feature_data.columns[0]: "number_of_models" }, inplace = True)


combined_feature_data = combined_feature_data.sort_values(['number_of_models'], ascending=[False])

combined_feature_data

from google.colab import files
combined_feature_data.to_csv('features.csv') 
files.download('features.csv')

"""### **Transformation - Federal reserve NY**"""

from google.colab import files
data_to_load = files.upload()

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import io
import types
import numpy as np
import pandas as pd
import statsmodels.api as sm

import matplotlib.pyplot as plt
import seaborn as sns
import math

def transform(column, transforms):
    transformation = transforms[column.name]
    # For quarterly data like GDP, we will compute
    # annualized percent changes
    mult = 4 if column.index.freqstr[0] == 'Q' else 1
    
    # 1 => No transformation
    if transformation == 1:
        pass
    # 2 => First difference
    elif transformation == 2:
        column = column.diff()
    # 3 => Second difference
    elif transformation == 3:
        column = column.diff().diff()
    # 4 => Log
    elif transformation == 4:
        column = np.log(column)
    # 5 => Log first difference, multiplied by 100
    #      (i.e. approximate percent change)
    elif transformation == 5:
        column = np.log(column).diff() * 100
    # 6 => Log second difference, multiplied by 100
    elif transformation == 6:
        column = np.log(column).diff().diff() * 100
    # 7 => Exact percent change, multiplied by 100
    #      with optional annualization
    elif transformation == 7:
        column = ((column / column.shift(1))**mult - 1.0) * 100
    # 8 => Log first difference, multiplied by 100
    #      (i.e. approximate percent change)
    #      with optional multiplier for annualization
    elif transformation == 8:
        column = np.log(column).diff() * 100 * mult
    # 9 => Log second difference, multiplied by 100
    #      with optional multiplier for annualization
    elif transformation == 9:
        column = np.log(column).diff().diff() * 100 * mult
    # 10 => Exact percent change, multiplied by 100
    #      with optional annualization
    elif transformation == 10:
        column = ((column / column.shift(1)) - 1.0) * 100
    elif transformation == 11:
        column = np.arcsinh(column).diff() * 100
    elif transformation == 12:
        column = np.arccosh(column).diff() * 100
    elif transformation == 13:
        column = np.arctanh(column).diff() * 100

      
    return column

def load_transformation():
        
    
    # 1. Download data
    #orig_m = (pd.read_csv(f'{base_url}/monthly/{vintage}.csv').dropna(how='all'))
    orig_m = (pd.read_csv(io.BytesIO(data_to_load['gdp_arcsinh.csv']),parse_dates=["DATE"]))
    
    # 2. Extract transformation information
    transform_m = orig_m.iloc[0, 1:]
    orig_m = orig_m.iloc[1:]

    # 3. Extract the date as an index
    orig_m.index = pd.PeriodIndex(orig_m.DATE.tolist(), freq='M')
    orig_m.drop('DATE', axis=1, inplace=True)

    # 4. Apply the transformations
    dta_m = orig_m.apply(transform, axis=0, transforms=transform_m)

    orig_m=orig_m
    dta_m=dta_m
    transform_m=transform_m

    df =pd.DataFrame(dta_m)

    #return types.SimpleNamespace(orig_m=orig_m, dta_m=dta_m, )    

    return df

df = load_transformation()
df

df.isnull().sum()

df_tra = df.copy()
df_tra.shape

# making new data frame with dropped NA values 
new_df = df_tra.dropna(axis = 0, how ='any') 
    
new_df

from google.colab import files
data_to_load = files.upload()

import pandas as pd
from matplotlib import pyplot as plt
from statsmodels.graphics.tsaplots import plot_acf

# data setup
import io
data = pd.read_csv(io.BytesIO(data_to_load['gdp_yoy.csv']),parse_dates=["DATE"])
data.head()

df= data.copy()

# creating features and label variable
X = df[['ALRCB','CIC','CPI','CRUDEBRUSD','EXSITC0','EXSITC1','EXSITC2','EXSITC3','EXSITC4','EXSITC5','EXSITC6','EXSITC7','EXSITC8','EXSITC9','EXTOT','IMSITC0','IMSITC1','IMSITC2','IMSITC3','IMSITC4','IMSITC5','IMSITC6','IMSITC7','IMSITC8','IMSITC9','IMTOT','IPIELEC','IPIMFG','IPIMIN','IPITOT','LGC','LIC','TOURIST','USDEXC','SOPC','SALETAX','SERVTAX','CONSCRE','RUBBER','PALMOIL','IMPCON','OBOCC','RETRENCH','ACTIVEJOBS','JOBVACAN']]
y = df[["GDPGR"]]

# Function to check stationarity

def adfuller_test(series, signif=0.05, name='', verbose=False):
    """Perform ADFuller to test for Stationarity of given series and print report"""
    r = adfuller(series, autolag='AIC')
    output = {'test_statistic':round(r[0], 4), 'pvalue':round(r[1], 4), 'n_lags':round(r[2], 4), 'n_obs':r[3]}
    p_value = output['pvalue'] 
    def adjust(val, length= 6): return str(val).ljust(length)

    # Print Summary
    print(f'    Augmented Dickey-Fuller Test on "{name}"', "\n   ", '-'*47)
    print(f' Null Hypothesis: Data has unit root. Non-Stationary.')
    print(f' Significance Level    = {signif}')
    print(f' Test Statistic        = {output["test_statistic"]}')
    print(f' No. Lags Chosen       = {output["n_lags"]}')

    for key,val in r[4].items():
        print(f' Critical value {adjust(key)} = {round(val, 3)}')

    if p_value <= signif:
        print(f" => P-Value = {p_value}. Rejecting Null Hypothesis.")
        print(f" => Series is Stationary.")
    else:
        print(f" => P-Value = {p_value}. Weak evidence to reject the Null Hypothesis.")
        print(f" => Series is Non-Stationary.")

# Data Prep

AI_Model_data = pd.concat([y, X], axis=1, sort=False)
#AI_Model_data = AI_Model_data.dropna()
# Check stationarity

# ADF Test on each column
for name, column in AI_Model_data.iteritems():
    adfuller_test(column, name=column.name)
    print('\n')

#Save transformation
from google.colab import files
new_df.to_csv('gdp_arcsinh.csv') 
files.download('gdp_arcsinh.csv')

from google.colab import files
data_to_load = files.upload()

import pandas as pd
from matplotlib import pyplot as plt
from statsmodels.graphics.tsaplots import plot_acf

# data setup
import io
data = pd.read_csv(io.BytesIO(data_to_load['gdp_arcsinh.csv']),parse_dates=["DATE"])
data.head()

import plotly.graph_objects as go
fig = go.Figure()
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,2], name = data.iloc[:,2].name, line = dict(color = '#FF0000'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,3], name = data.iloc[:,3].name, line = dict(color = '#CD853F'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,4], name = data.iloc[:,4].name, line = dict(color = '#483D8B'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,5], name = data.iloc[:,5].name, line = dict(color = '#00008B'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,6], name = data.iloc[:,6].name, line = dict(color = '#8B4513'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,7], name = data.iloc[:,7].name, line = dict(color = '#7B68EE'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,8], name = data.iloc[:,8].name, line = dict(color = '#800080'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,9], name = data.iloc[:,9].name, line = dict(color = '#87CEEB'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,10], name = data.iloc[:,10].name, line = dict(color = '#FFDAB9'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,11], name = data.iloc[:,11].name, line = dict(color = '#32CD32'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,12], name = data.iloc[:,12].name, line = dict(color = '#D8BFD8'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,13], name = data.iloc[:,13].name, line = dict(color = '#DDA0DD'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,14], name = data.iloc[:,14].name, line = dict(color = '#6495ED'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,15], name = data.iloc[:,15].name, line = dict(color = '#FFA07A'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,16], name = data.iloc[:,16].name, line = dict(color = '#4682B4'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,17], name = data.iloc[:,17].name, line = dict(color = '#556B2F'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,18], name = data.iloc[:,18].name, line = dict(color = '#9ACD32'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,19], name = data.iloc[:,19].name, line = dict(color = '#B0C4DE'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,20], name = data.iloc[:,20].name, line = dict(color = '#9370DB'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,21], name = data.iloc[:,21].name, line = dict(color = '#DAA520'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,22], name = data.iloc[:,22].name, line = dict(color = '#0000CD'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,23], name = data.iloc[:,23].name, line = dict(color = '#008080'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,24], name = data.iloc[:,24].name, line = dict(color = '#B8860B'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,25], name = data.iloc[:,25].name, line = dict(color = '#FFD700'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,26], name = data.iloc[:,26].name, line = dict(color = '#800000'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,27], name = data.iloc[:,27].name, line = dict(color = '#FF8C00'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,28], name = data.iloc[:,28].name, line = dict(color = '#B0E0E6'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,29], name = data.iloc[:,29].name, line = dict(color = '#FF00FF'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,30], name = data.iloc[:,30].name, line = dict(color = '#F4A460'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,31], name = data.iloc[:,31].name, line = dict(color = '#FFFF00'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,32], name = data.iloc[:,32].name, line = dict(color = '#D2691E'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,33], name = data.iloc[:,33].name, line = dict(color = '#7FFFD4'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,34], name = data.iloc[:,34].name, line = dict(color = '#0000FF'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,35], name = data.iloc[:,35].name, line = dict(color = '#FFEFD5'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,36], name = data.iloc[:,36].name, line = dict(color = '#EEE8AA'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,37], name = data.iloc[:,37].name, line = dict(color = '#FFA500'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,38], name = data.iloc[:,38].name, line = dict(color = '#9400D3'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,39], name = data.iloc[:,39].name, line = dict(color = '#FFDEAD'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,40], name = data.iloc[:,40].name, line = dict(color = '#66CDAA'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,41], name = data.iloc[:,41].name, line = dict(color = '#8B008B'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,42], name = data.iloc[:,42].name, line = dict(color = '#D2B48C'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,43], name = data.iloc[:,43].name, line = dict(color = '#ADFF2F'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,44], name = data.iloc[:,44].name, line = dict(color = '#008000'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,45], name = data.iloc[:,45].name, line = dict(color = '#F5DEB3'), opacity = 0.8))
fig.add_trace(go.Scatter(x=data.DATE, y=data.iloc[:,46], name = data.iloc[:,46].name, line = dict(color = '#000000'), opacity = 1.0))